diff --git a/llvm/lib/CodeGen/AtomicExpandPass.cpp b/llvm/lib/CodeGen/AtomicExpandPass.cpp
index fca7a5240ffc..925d740478f9 100644
--- a/llvm/lib/CodeGen/AtomicExpandPass.cpp
+++ b/llvm/lib/CodeGen/AtomicExpandPass.cpp
@@ -14,6 +14,8 @@
 //
 //===----------------------------------------------------------------------===//
 
+#include <iostream>
+
 #include "llvm/ADT/ArrayRef.h"
 #include "llvm/ADT/STLFunctionalExtras.h"
 #include "llvm/ADT/SmallVector.h"
@@ -210,24 +212,28 @@ bool AtomicExpand::runOnFunction(Function &F) {
     // If the Size/Alignment is not supported, replace with a libcall.
     if (LI) {
       if (!atomicSizeSupported(TLI, LI)) {
+        std::cout << "[bsarden][AtomicExpand] expanding Atomic Load to libcall" << std::endl;
         expandAtomicLoadToLibcall(LI);
         MadeChange = true;
         continue;
       }
     } else if (SI) {
       if (!atomicSizeSupported(TLI, SI)) {
+        std::cout << "[bsarden][AtomicExpand] expanding Atomic Store to libcall" << std::endl;
         expandAtomicStoreToLibcall(SI);
         MadeChange = true;
         continue;
       }
     } else if (RMWI) {
       if (!atomicSizeSupported(TLI, RMWI)) {
+        std::cout << "[bsarden][AtomicExpand] expanding Atomic RMW to libcall" << std::endl;
         expandAtomicRMWToLibcall(RMWI);
         MadeChange = true;
         continue;
       }
     } else if (CASI) {
       if (!atomicSizeSupported(TLI, CASI)) {
+        std::cout << "[bsarden][AtomicExpand] expanding Atomic CAS to libcall" << std::endl;
         expandAtomicCASToLibcall(CASI);
         MadeChange = true;
         continue;
@@ -286,7 +292,10 @@ bool AtomicExpand::runOnFunction(Function &F) {
         CASI->setFailureOrdering(AtomicOrdering::Monotonic);
       }
 
-      if (FenceOrdering != AtomicOrdering::Monotonic) {
+      // if (FenceOrdering != AtomicOrdering::Monotonic) {
+      if (true) {
+        std::cout << "[bsarden][AtomicExpand]: Inserting a fence!" << std::endl;
+        // Insert fences even for the basic `std::memory_order_relaxed` case.
         MadeChange |= bracketInstWithFences(I, FenceOrdering);
       }
     }
@@ -440,6 +449,8 @@ bool AtomicExpand::tryExpandAtomicStore(StoreInst *SI) {
 bool AtomicExpand::expandAtomicLoadToLL(LoadInst *LI) {
   ReplacementIRBuilder Builder(LI, *DL);
 
+  std::cout << "[bsarden][AtomicExpand]: expandAtomicLoadToLL" << std::endl;
+
   // On some architectures, load-linked instructions are atomic for larger
   // sizes than normal loads. For example, the only 64-bit load guaranteed
   // to be single-copy atomic by ARM is an ldrexd (A3.5.3).
@@ -1203,6 +1214,8 @@ bool AtomicExpand::expandAtomicCmpXchg(AtomicCmpXchgInst *CI) {
   // care of everything. Otherwise, emitLeading/TrailingFence are no-op and we
   // should preserve the ordering.
   bool ShouldInsertFencesForAtomic = TLI->shouldInsertFencesForAtomic(CI);
+  std::cout << "[bsarden][AtomicExpand] ShouldInsertFencesForAtomic: " << ShouldInsertFencesForAtomic << "\n";
+  // LLVM_DEBUG(dbgs() << "ShouldInsertFencesForAtomic: " << ShouldInsertFencesForAtomic << "\n");
   AtomicOrdering MemOpOrder = ShouldInsertFencesForAtomic
                                   ? AtomicOrdering::Monotonic
                                   : CI->getMergedOrdering();
diff --git a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
index da1d2140d5cf..5ac3318f4cfc 100644
--- a/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
+++ b/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp
@@ -10,6 +10,8 @@
 //
 //===----------------------------------------------------------------------===//
 
+#include <iostream>
+
 #include "AArch64ISelLowering.h"
 #include "AArch64CallingConvention.h"
 #include "AArch64ExpandImm.h"
@@ -21650,7 +21652,9 @@ bool AArch64TargetLowering::isOpSuitableForLDPSTP(const Instruction *I) const {
 
 bool AArch64TargetLowering::shouldInsertFencesForAtomic(
     const Instruction *I) const {
-  return isOpSuitableForLDPSTP(I);
+  // std::cout << "[bsarden][AArch64ISelLowering]: shouldInsertFencesForAtomic: " << isOpSuitableForLDPSTP(I) << std::endl;
+  // return isOpSuitableForLDPSTP(I);
+  return true;
 }
 
 // Loads and stores less than 128-bits are already atomic; ones above that
@@ -21659,6 +21663,7 @@ bool AArch64TargetLowering::shouldInsertFencesForAtomic(
 TargetLoweringBase::AtomicExpansionKind
 AArch64TargetLowering::shouldExpandAtomicStoreInIR(StoreInst *SI) const {
   unsigned Size = SI->getValueOperand()->getType()->getPrimitiveSizeInBits();
+  std::cout << "[bsarden][AArch64ISelLowering]: shouldExpandAtomicStoreInIR: size " << Size << std::endl;
   if (Size != 128 || isOpSuitableForLDPSTP(SI))
     return AtomicExpansionKind::None;
   return AtomicExpansionKind::Expand;
@@ -21670,6 +21675,7 @@ AArch64TargetLowering::shouldExpandAtomicStoreInIR(StoreInst *SI) const {
 TargetLowering::AtomicExpansionKind
 AArch64TargetLowering::shouldExpandAtomicLoadInIR(LoadInst *LI) const {
   unsigned Size = LI->getType()->getPrimitiveSizeInBits();
+  std::cout << "[bsarden][AArch64ISelLowering]: shouldExpandAtomicLoadInIR: size " << Size << std::endl;
 
   if (Size != 128 || isOpSuitableForLDPSTP(LI))
     return AtomicExpansionKind::None;
@@ -21692,6 +21698,7 @@ AArch64TargetLowering::shouldExpandAtomicRMWInIR(AtomicRMWInst *AI) const {
     return AtomicExpansionKind::CmpXChg;
 
   unsigned Size = AI->getType()->getPrimitiveSizeInBits();
+  std::cout << "[bsarden][AArch64ISelLowering]: shouldExpandAtomicRMWInIR: size " << Size << std::endl;
   if (Size > 128) return AtomicExpansionKind::None;
 
   // Nand is not supported in LSE.
